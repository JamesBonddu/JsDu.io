# 通过张量分解了解词嵌入的组成 

https://openreview.net/forum?id=H1eqjiCctX

Embedding过程  
Embedding 这样一种将离散变量转变为连续向量的方式为神经网络在各方面的应用带来了极大的扩展。该技术目前主要有两种应用，NLP 中常用的 word embedding 以及用于类别数据的 entity embedding。

Distributed representation可以解决One hot representation的问题，它最早是 Hinton 于 1986 年提出的。它的思路是通过训练，将每个词都映射到一个较短的词向量上来。所有的这些词向量就构成了向量空间，进而可以用普通的统计学的方法来研究词与词之间的关系。这就是word embedding（词嵌入），即指的是将词转化成一种分布式表示，又称词向量。

Embedding 最酷的一个地方在于它们可以用来可视化出表示的数据的相关性，当然要我们能够观察，需要通过降维技术来达到 2 维或 3 维。最流行的降维技术是：t-Distributed Stochastic Neighbor Embedding (TSNE)。

较著名的采用neural network language model生成词向量的方法有：Skip-gram、CBOW、LBL、NNLM、C&W、GloVe等

BOW（Continuous Bag-of-Words Model）：在已知上下文，例如wt−2,wt−1,wt+1,wt+2的前提下，如何预测当前词wt。学习的目标函数是最大化对数似然函数：
∑w∈Clogp(w|Context(w))

Skip-gram模型，使用中心词来预测上下文词。
kip-gram（Continuous Skip-gram Model）：在已知当前词wt的前提下，预测其上下文，例如wt−2,wt−1,wt+1,wt+2。目标函数形如：
∑w∈Clogp(Context(w)|w)

Skip-Gram模型和CBOW的思路是反着来的（互为镜像），即输入是特定的一个词的词向量（单词的one-hot编码），而输出是特定词对应的上下文词向量（所有上下文单词的one-hot编码）。还是上面的例子，我们的上下文大小取值为4， 特定的这个词”Learning”是我们的输入，而这8个上下文词是我们的输出。



https://zhuanlan.zhihu.com/p/46016518

https://projector.tensorflow.org/

Word2Vec

https://www.biaodianfu.com/google-word2vec.html

## torch 中Embedding的过程

Pytorch中的embedding层非常简单的调用，torch.nn.Embedding(m,n),其中m表示单词的总数目，n表示词嵌入的维度。

https://yifdu.github.io/2018/12/05/Embedding%E5%B1%82/

https://lhyxx.top/2020/07/21/pytorch%E4%B8%ADEmbedding%E7%9A%84%E4%BD%BF%E7%94%A8/