# æ•™ç¨‹

https://github.com/abhimishra91/pytorch-tutorials

git@github.com:abhimishra91/transformers-tutorials.git

https://hub.baai.ac.cn/

https://github.com/scutan90/DeepLearning-500-questions

https://github.com/microsoft/unilm


The notebook will be divided into seperate sections to provide a organized walk through for the process used. This process can be modified for individual use cases. The sections are:

æœºå™¨å­¦ä¹ çš„æµç¨‹
- Importing Python Libraries and preparing the environment
- Importing and Pre-Processing the domain data
- Preparing the Dataset and Dataloader
- Creating the Neural Network for Fine Tuning
- Fine Tuning the Model
- Validating the Model Performance
- Saving the model and artifacts for Inference in Future

# Loss Function and Optimizer
- Loss Function and Optimizer and defined in the next cell.
- The Loss Function is used the calculate the difference in the output created by the model and the actual output.
- Optimizer is used to update the weights of the neural network to improve its performance.


# Further Reading
- You can refer to my [Pytorch Tutorials](https://github.com/abhimishra91/pytorch-tutorials) to get an intuition of Loss Function and Optimizer.
- [Pytorch Documentation for Loss Function](https://pytorch.org/docs/stable/nn.html#loss-functions)
- [Pytorch Documentation for Optimizer](https://pytorch.org/docs/stable/optim.html)
- Refer to the links provided on the top of the notebook to read more about DistiBERT. 

æ–‡æœ¬åˆ†ç±»å­—æ®µåç§°:
Index(['id', 'comment_text', 'toxic', 'severe_toxic', 'obscene', 'threat',
       'insult', 'identity_hate', 'list'],
      dtype='object')

	comment_text	list
159566	":::::And for the second time of asking, when ...	[0, 0, 0, 0, 0, 0]
159567	You should be ashamed of yourself \n\nThat is ...	[0, 0, 0, 0, 0, 0]
159568	Spitzer \n\nUmm, theres no actual article for ...	[0, 0, 0, 0, 0, 0]
159569	And it looks like it was actually you who put ...	[0, 0, 0, 0, 0, 0]
159570	"\nAnd ... I really don't think you understand...	[0, 0, 0, 0, 0, 0]


ç‚ºäº†è®“ä½ ç›´è§€äº†è§£ BERT é‹ä½œï¼Œæœ¬æ–‡ä½¿ç”¨åŒ…å«ç¹é«”èˆ‡ç°¡é«”ä¸­æ–‡çš„é è¨“ç·´æ¨¡å‹ã€‚ ä½ å¯ä»¥åœ¨ Hugging Face åœ˜éšŠçš„ repo è£¡çœ‹åˆ°æ‰€æœ‰å¯å¾ PyTorch Hub è¼‰å…¥çš„ BERT é è¨“ç·´æ¨¡å‹ã€‚æˆªè‡³ç›®å‰ç‚ºæ­¢æœ‰ä»¥ä¸‹æ¨¡å‹å¯ä¾›ä½¿ç”¨ï¼š

- bert-base-chinese
- bert-base-uncased
- bert-base-cased
- bert-base-german-cased
- bert-base-multilingual-uncased
- bert-base-multilingual-cased
- bert-large-cased
- bert-large-uncased
- bert-large-uncased-whole-word-masking
- bert-large-cased-whole-word-masking



## åœç”¨è¯è¡¨ stopwords

BERT è£¡é ­æœ‰ 5 å€‹ç‰¹æ®Š tokens å„å¸å…¶è·ï¼š

- [CLS]ï¼šåœ¨åšåˆ†é¡ä»»å‹™æ™‚å…¶æœ€å¾Œä¸€å±¤çš„ repr. æœƒè¢«è¦–ç‚ºæ•´å€‹è¼¸å…¥åºåˆ—çš„ repr.
- [SEP]ï¼šæœ‰å…©å€‹å¥å­çš„æ–‡æœ¬æœƒè¢«ä¸²æ¥æˆä¸€å€‹è¼¸å…¥åºåˆ—ï¼Œä¸¦åœ¨å…©å¥ä¹‹é–“æ’å…¥é€™å€‹ token ä»¥åšå€éš”
- [UNK]ï¼šæ²’å‡ºç¾åœ¨ BERT å­—å…¸è£¡é ­çš„å­—æœƒè¢«é€™å€‹ token å–ä»£
- [PAD]ï¼šzero padding é®ç½©ï¼Œå°‡é•·åº¦ä¸ä¸€çš„è¼¸å…¥åºåˆ—è£œé½Šæ–¹ä¾¿åš batch é‹ç®—
- [MASK]ï¼šæœªçŸ¥é®ç½©ï¼Œåƒ…åœ¨é è¨“ç·´éšæ®µæœƒç”¨åˆ°

https://github.com/goto456/stopwords


# åˆ†è¯

Among all the tokenization algorithms, we can highlight a few subtokens algorithms used in Transformers-based SoTA models : 

- [Byte Pair Encoding (BPE) - Neural Machine Translation of Rare Words with Subword Units (Sennrich et al., 2015)](https://arxiv.org/abs/1508.07909)
- [Word Piece - Japanese and Korean voice search (Schuster, M., and Nakajima, K., 2015)](https://research.google/pubs/pub37842/)
- [Unigram Language Model - Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates (Kudo, T., 2018)](https://arxiv.org/abs/1804.10959)
- [Sentence Piece - A simple and language independent subword tokenizer and detokenizer for Neural Text Processing (Taku Kudo and John Richardson, 2018)](https://arxiv.org/abs/1808.06226)

For each of the components above we provide multiple implementations:

- **Normalizer**: Lowercase, Unicode (NFD, NFKD, NFC, NFKC), Bert, Strip, ...
- **PreTokenizer**: ByteLevel, WhitespaceSplit, CharDelimiterSplit, Metaspace, ...
- **Model**: WordLevel, BPE, WordPiece
- **Post-Processor**: BertProcessor, ...
- **Decoder**: WordLevel, BPE, WordPiece, ...

# BERT fintune

https://zhuanlan.zhihu.com/p/62642374

https://leemeng.tw/attack_on_bert_transfer_learning_in_nlp.html

# practice 

```python
# åˆ†è¯
token_ids = tokens['input_ids'][i]
token_str = [tokenizer.convert_ids_to_tokens(s) for s in tokens['input_ids'][i]]

input_tf = tokenizer("This is a sample input", return_tensors="tf")
"""
If set, will return tensors instead of list of python integers. Acceptable values are:

'tf': Return TensorFlow tf.constant objects.

'pt': Return PyTorch torch.Tensor objects.

'np': Return Numpy np.ndarray objects.
"""

```

# piplines

**pipelines** provides a high-level, easy to use,
API for doing inference over a variety of downstream-tasks, including: 

- ***Sentence Classification _(Sentiment Analysis)_***: Indicate if the overall sentence is either positive or negative, i.e. *binary classification task* or *logitic regression task*.
- ***Token Classification (Named Entity Recognition, Part-of-Speech tagging)***: For each sub-entities _(*tokens*)_ in the input, assign them a label, i.e. classification task.
- ***Question-Answering***: Provided a tuple (`question`, `context`) the model should find the span of text in `content` answering the `question`.
- ***Mask-Filling***: Suggests possible word(s) to fill the masked input with respect to the provided `context`.
- ***Summarization***: Summarizes the ``input`` article to a shorter article.
- ***Translation***: Translates the input from a language to another language.
- ***Feature Extraction***: Maps the input to a higher, multi-dimensional space learned from the data.

Pipelines encapsulate the overall process of every NLP process:
 
 1. *Tokenization*: Split the initial input into multiple sub-entities with ... properties (i.e. tokens).
 2. *Inference*: Maps every tokens into a more meaningful representation. 
 3. *Decoding*: Use the above representation to generate and/or extract the final output for the underlying task.

# ONNX

[ONNX](http://onnx.ai/) is open format for machine learning models. It allows to save your neural network's computation graph in a framework agnostic way, which might be particulary helpful when deploying deep learning models.

å¯¼å‡ºæˆONNXæ ¼å¼
```python
from pathlib import Path
from transformers.convert_graph_to_onnx import convert

# Handles all the above steps for you
convert(framework="pt", model="bert-base-cased", output=Path("onnx/bert-base-cased.onnx"), opset=11)
```

Exporting models (either PyTorch or TensorFlow) is easily achieved through the conversion tool provided as part of ğŸ¤— transformers repository.

Under the hood the process is sensibly the following:

- Allocate the model from transformers (PyTorch or TensorFlow)
- Forward dummy inputs through the model this way ONNX can record the set of operations executed
- Optionally define dynamic axes on input and output tensors
- Save the graph along with the network parameters
