# 机器学习

监督学习 supervised learning;
非监督学习 unsupervised learning;
半监督学习 semi-supervised learning;
强化学习 reinforcement learning;
遗传算法 genetic algorithm.

神经网络分很多种, 有普通的前向传播神经网络 , 有分析图片的 CNN 卷积神经网络 , 有分析序列化数据, 比如语音的 RNN 循环神经网络 , 这些神经网络都是用来输入数据, 得到想要的结果, 我们看中的是这些神经网络能很好的将数据与结果通过某种关系联系起来.

> 反向传播 Backpropagation

https://www.cnblogs.com/charlotte77/p/5629865.html

https://www.jiqizhixin.com/graph/technologies/7332347c-8073-4783-bfc1-1698a6257db3

https://www.zhihu.com/question/27239198

https://yongyuan.name/blog/back-propagtion.html

> Dropout 缓解过拟合

数据量少的时候比较容易出现过拟合的现象, 在训练模型时加上dropout层可以缓解过拟合现象.

> 梯度下降 Gradient Descent

https://zh.wikipedia.org/wiki/%E6%A2%AF%E5%BA%A6%E4%B8%8B%E9%99%8D%E6%B3%95

> 激励函数 activation function

激励函数必须是可以微分的, 因为在 backpropagation 误差反向传递的时候, 只有这些可微分的激励函数才能把误差传递回去.

> 损失函数 Loss Function 

损失函数用来评价模型的预测值和真实值不一样的程度，损失函数越好，通常模型的性能越好。

损失函数分为经验风险损失函数和结构风险损失函数。经验风险损失函数指预测结果和实际结果的差别，结构风险损失函数是指经验风险损失函数加上正则项。

> 对数似然函数

https://blog.csdn.net/itplus/article/details/19169027

> 决策树

LR模型是一股脑儿的把所有特征塞入学习，而决策树更像是编程语言中的if-else一样，去做条件判断，这就是根本性的区别。

https://github.com/NLP-LOVE/ML-NLP/blob/master/Machine%20Learning/3.Desition%20Tree/Desition%20Tree.md

ID3算法【计算信息增益的算法】
在根节点处计算信息熵，然后根据属性依次划分并计算其节点的信息熵，用根节点信息熵--属性节点的信息熵=信息增益，根据信息增益进行降序排列，排在前面的就是第一个划分属性，其后依次类推，这就得到了决策树的形状，也就是怎么“长”了。
信息增益有一个问题：对可取值数目较多的属性有所偏好，例如：考虑将“编号”作为一个属性。为了解决这个问题，引出了另一个 算法C4.5。

C4.5
信息增益比本质： 是在信息增益的基础之上乘上一个惩罚参数。特征个数较多时，惩罚参数较小；特征个数较少时，惩罚参数较大。**不过有一个缺点：
缺点：信息增益率偏向取值较少的特征。

CART
表示在样本集合中一个随机选中的样本被分错的概率。举例来说，现在一个袋子里有3种颜色的球若干个，伸手进去掏出2个球，颜色不一样的概率，这下明白了吧。Gini(D)越小，数据集D的纯度越高。


树形结构为什么不需要归一化?

因为数值缩放不影响分裂点位置，对树模型的结构不造成影响。 按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。而且，树模型是不能进行梯度下降的，因为构建树模型（回归树）寻找最优点时是通过寻找最优分裂点完成的，因此树模型是阶跃的，阶跃点是不可导的，并且求导没意义，也就不需要归一化。

既然树形结构（如决策树、RF）不需要归一化，那为何非树形结构比如Adaboost、SVM、LR、Knn、KMeans之类则需要归一化。

对于线性模型，特征值差别很大时，运用梯度下降的时候，损失等高线是椭圆形，需要进行多次迭代才能到达最优点。 但是如果进行了归一化，那么等高线就是圆形的，促使SGD往原点迭代，从而导致需要的迭代次数较少。


https://www.jiqizhixin.com/articles/2018-06-21-3

https://zh.wikipedia.org/wiki/%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0

https://keras.io/zh/losses/



https://mofanpy.com/tutorials/machine-learning/

https://mofanpy.com/tutorials/machine-learning/tensorflow/

http://www.jianshu.com/p/e112012a4b2d

https://mofanpy.com/tutorials/machine-learning/theano/

https://mofanpy.com/tutorials/machine-learning/keras/

数据集

https://github.com/tensorflow/datasets/tree/v3.2.1

pytorch开发调试

https://mofanpy.com/tutorials/machine-learning/torch/

https://www.jiqizhixin.com/articles/2019-06-18-10

# 卷积神经网络

卷积神经网络包含输入层、隐藏层和输出层，隐藏层又包含卷积层和pooling层，图像输入到卷积神经网络后通过卷积来不断的提取特征，每提取一个特征就会增加一个feature map

# CNN

流行的CNN结构

从下到上的顺序, 首先是输入的图片(image), 经过一层卷积层 (convolution), 然后在用池化(pooling)方式处理卷积的信息, 这里使用的是 max pooling 的方式. 然后在经过一次同样的处理, 把得到的第二次处理的信息传入两层全连接的神经层 (fully connected),这也是一般的两层神经网络层,最后在接上一个分类器(classifier)进行分类预测. 这仅仅是对卷积神经网络在图片处理上一次简单的介绍. 如果你想知道使用 python 搭建这样的卷积神经网络, 欢迎点击下面的内容.

# RNN

## LSTM

LSTM 是 long-short term memory 的简称, 中文叫做 长短期记忆. 是当下最流行的 RNN 形式之一;
LSTM 就是为了解决这个问题而诞生的. LSTM 和普通 RNN 相比, 多出了三个控制器. (输入控制, 输出控制, 忘记控制). 

http://colah.github.io/posts/2015-08-Understanding-LSTMs/

https://mofanpy.com/tutorials/machine-learning/ML-intro/RNN/

# autoencoder

https://mofanpy.com/tutorials/machine-learning/ML-intro/autoencoder/

# GAN

生成网络, 叫做 GAN, 又称生成对抗网络, 也是 Generative Adversarial Nets 的简称.

# 迁移学习
https://cs231n.github.io/transfer-learning/




优秀的博客

https://www.bilibili.com/video/BV1Ux411j7ri

http://colah.github.io/

https://distill.pub/

https://www.patreon.com/

# 常用的优化器 Optimizer

利用损失函数 lossfunction 来检验算法模型的优劣，同时利用损失函数来提升算法模型．
这个提升的过程就叫做优化(Optimizer)

## adam

https://machinelearningjourney.com/index.php/2021/01/09/adam-optimizer/

https://www.cnblogs.com/GeekDanny/p/9655597.html

# dropout层
是指在随机选择的某些神经元集合的训练阶段忽略单位（即神经元）。“忽略”是指在特定的向前或向后通过过程中不考虑这些单位。
目的是为了防止过度拟合. 完全连接的层占据了大多数参数，因此，神经元在训练过程中彼此之间发展了相互依赖性，这抑制了每个神经元的个体力量，导致训练数据过度拟合。

https://medium.com/@amarbudhiraja/https-medium-com-amarbudhiraja-learning-less-to-learn-better-dropout-in-deep-machine-learning-74334da4bfc5


# SoftMax 归一化函数

Softmax函数，或称归一化指数函数[1]:198，是逻辑函数的一种推广。它能将一个含任意实数的K维向量 {\displaystyle \mathbf {z} }{\displaystyle \mathbf {z} } “压缩”到另一个K维实向量 {\displaystyle \sigma (\mathbf {z} )}{\displaystyle \sigma (\mathbf {z} )} 中，使得每一个元素的范围都在{\displaystyle (0,1)}{\displaystyle (0,1)}之间，并且所有元素的和为1(也可视为一个 (k-1)维的hyperplane或subspace)。

https://zh.wikipedia.org/wiki/Softmax%E5%87%BD%E6%95%B0

# 激活函数
sigmoid, tanh, relu
激活函数（activation functions）的目标是，将神经网络非线性化。激活函数是连续的（continuous），且可导的（differential）。

https://www.jianshu.com/p/857d5859d2cc