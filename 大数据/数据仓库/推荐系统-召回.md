# 召回


## 多模态召回

https://tech.meituan.com/2020/09/27/
kdd-cup-multimodalities-recall-03.html

## 向量召回

https://www.6aiq.com/article/1601292770313


https://www.cklin.top/post/rs-greater-tui-jian-xi-tong-san-chi-xian-zhao-hui-yu-pai-xu/

推荐系统踩坑

https://zhuanlan.zhihu.com/p/337982340


爱奇艺推荐和召回
http://www.woshipm.com/pd/847004.html

pipeline构建过程

https://www.6aiq.com/article/1578982502207

## surprise推荐系统

http://surpriselib.com/

https://surprise.readthedocs.io/en/stable/

https://github.com/yzsunlei/yzsunlei.github.io/blob/master/_posts/%E7%BC%96%E7%A8%8B/2019-03-03-%E6%8E%A8%E8%8D%90%E7%B3%BB%E7%BB%9F%E6%9D%82%E4%B8%83%E6%9D%82%E5%85%AB%E7%9A%84.md


# 多路召回

https://zhuanlan.zhihu.com/p/321597287

https://www.youtube.com/watch?v=ZQ1YEbFSyIQ

https://www.tvnnu.com/bianchengjishu/22194.html

https://www.git2get.com/av/105239826.html

# 推荐系统召回入门系列视频

题主的问题，以及上图的图示，其实都是一条边或者两条边，如下给下大概的解释i2i：计算item-item相似度，用于相似推荐、相关推荐、关联推荐；
u2i：基于矩阵分解、协同过滤的结果，直接给u推荐i；
u2u2i：基于用户的协同过滤，先找相似用户，再推荐相似用户喜欢的item；
u2i2i：基于物品的协同过滤，先统计用户喜爱的物品，再推荐他喜欢的物品；
u2tag2i：基于标签的泛化推荐，先统计用户偏好的tag向量，然后匹配所有的Item，这个tag一般是item的标签、分类、关键词等tag;

https://www.youtube.com/watch?v=BMtEiFKEKvE&list=PLCemT-oocgalODXpQ-EP_IfrrD-A--40h&index=3

https://zhuanlan.zhihu.com/p/260212015

https://www.zhihu.com/question/291559021

https://www.pianshen.com/article/22631558066/

https://www.cnblogs.com/by-dream/p/10450880.html

## fasttext

http://fasttext.apachecn.org/#/

# 深度学习推荐

https://www.6aiq.com/article/1584946350988

## i2i推荐

https://tech.meituan.com/2020/08/20/kdd-cup-debiasing-practice.html

https://www.upyun.com/tech/article/617/%E6%9C%89%E8%B5%9E%E4%B8%AA%E6%80%A7%E5%8C%96%E6%8E%A8%E8%8D%90%E8%83%BD%E5%8A%9B%E7%9A%84%E6%BC%94%E8%BF%9B%E4%B8%8E%E5%AE%9E%E8%B7%B5.html

https://xueqiu.com/9217191040/160568287

https://www.infoq.cn/article/0gmqhyrh0gurukap92u1

# 词向量存储格式

有三种存储格式：

txt
文本格式，类似 word 0.001233 0.34219 …
bin
google的序列化，二进制模式；
mmap
内存共享模式。一个字就是快；加载快

https://blog.csdn.net/iterate7/article/details/88938277

> word2vec文本格式

1 300
word -0.0762464299711 0.0128308048976 ... 0.0712385589283

https://stackoverflow.com/questions/49750112/gensim-how-to-load-precomputed-word-vectors-from-text-file

## gensim词向量使用

```python
#coding:utf-8
import gensim
from gensim.models import KeyedVectors

word2vec_model_path = './data/data_vec.txt' ##词向量文件的位置
word2vec_model = KeyedVectors.load_word2vec_format(word2vec_model_path, binary=False,unicode_errors='ignore')
word2vec_dict = {}
for word, vector in zip(word2vec_model.vocab, word2vec_model.vectors):
    if '.bin' not in word2vec_model_path:
        word2vec_dict[word] = vector
    else:
        word2vec_dict[word] = vector /np.linalg.norm(vector) 
for each in word2vec_dict:
    print (each,word2vec_dict[each])
```

https://blog.csdn.net/yangfengling1023/article/details/81705109

https://blog.csdn.net/u010700066/article/details/83070102

https://blog.csdn.net/lilong117194/article/details/82849054

https://www.jianshu.com/p/bba1bf9518dc

http://codewithzhangyi.com/2019/12/11/using-pre-trained-word-embeddings-in-keras/

### 加速gensim使用

```python
方法 1：缩减数据集
   原始的数据集解压后有将近 16 个 G 的大小，包含 800 万个中文词汇，但是在大多数场景下，尤其是实际应用场景下，我们用到的可能也就 10 万个左右，在加载的时候可以加上 limit 参数限制数量。我设置了 limit=10000，加载的时间不超过 2 秒。

from gensim.models import KeyedVectors

file = 'data/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'
wv_from_text = KeyedVectors.load_word2vec_format (file, binary=False, limit=100000)
方法 2：保存模型
   使用语料库导入到 word2vec 模型实际上我们得到的是一个前向推理的神经网络，如果我们不训练只使用这个网络做推导，在后续的使用过程中该网络的参数是不会改变的，因此我们可以保存这些参数下来，一个网络中的参数的大小肯定是远小于整个语料库的大小。而且，word2vec 模型直接加载参数肯定比从数据集中构建要快得多。我尝试保存了 10 万条数据的网络，大小不超过 5MB。如果我们想用在实时应用，我们可以先在一台性能比较好的机器上将数据导入，保存好模型参数，然后把这个参数文件放到服务器上，服务启动的时候加载就可以了。

from gensim.models import KeyedVectors

file = 'data/Tencent_AILab_ChineseEmbedding/Tencent_AILab_ChineseEmbedding.txt'
wv_from_text = KeyedVectors.load_word2vec_format (file, binary=False, limit=100000)
wv_from_text.init_sims (replace=True) # save memory to run faster

# save model for laster use
wv_from_text.save ('./test.bin')

# load model from bin file
model = KeyedVectors.load ('./test.bin')
```

http://leungyukshing.cn/archives/Gensim%E5%BF%AB%E9%80%9F%E5%8A%A0%E8%BD%BD%E6%95%B0%E6%8D%AE%E9%9B%86.html

https://stackoverflow.com/questions/42986405/how-to-speed-up-gensim-word2vec-model-load-time

Tencent AI Lab Embedding Corpus for Chinese Words and Phrases

https://ai.tencent.com/ailab/nlp/en/embedding.html

腾讯词向量使用

https://blog.csdn.net/Suan2014/article/details/83184244


## 词嵌入评测

https://wefe.readthedocs.io/en/latest/about.html


## 个性推荐

https://www.msra.cn/zh-cn/news/executivebylines/tech-bylines-personalized-recommendation-system

https://blog.csdn.net/Lynnzxl/article/details/105224788

## 推荐算法调研

http://xtf615.com/2018/05/03/recommender-system-survey/

https://www.dataapplab.com/algorithm-principle-overview-of-recommender-systems/



### 协同推荐算法

推荐系统下常常提到的一个算法就是协同过滤，我们在之前的文章当中也曾经详细写过它的原理，这里我们就不赘述了。有需要的同学可以点击下方的传送门回顾一下。这里我主要想要分享一下协同过滤的使用和特性。

SVD | 简介推荐场景中的协同过滤算法，以及 SVD 的使用

协同过滤在很长一段时间内被认为等价于推荐算法，其实这是不对的，协同过滤目前应用最广的就是一种召回的策略。它最大的用处是召回相似的商品，本质上这是一种计算商品相似度的一种算法。怎么计算商品的相似度呢？通过用户来计算，如果若干商品被类似的用户发生过行为，那么就认为它们是相似的。

但是这个只是感性的认识，对于算法而言，我们需要明确的指标以及计算方法。所以我们把用户的行为抽象成向量，通过计算向量的相似度来计算商品之间的相似度，这当然是一种近似，而且是一种粒度很粗的近似，但是对于大多数不能直接计算相似度的场景而言，这样的方式的效果还是很不错的。

协同过滤一般主要分为两种，一种叫做 i2i，一种叫做 u2i。i2i 即 item to item，也就是计算 item 之间的相似度，寻找相似 item。u2i 则是 user to item，根据用户向量寻找和用户向量相似的 item，这种应用得比较少一些。还有一种稍微冷门一些的叫做 u2u2i，也就是先找到用户的相似用户，然后再找到相似用户喜欢的 item。

https://lumingdong.cn/cooperative-recommendation-algorithms.html

## 相关竞品

https://www.sensorsdata.cn/product/recommend.html

https://www.infoq.cn/article/jb3_dhjgrcron174ucv6

# 经典论文

https://netflixtechblog.com/system-architectures-for-personalization-and-recommendation-e081aa94b5d8


# 文档如何转换成向量

https://blog.csdn.net/flyfrommath/article/details/79643233


# milvus 处理文档数据转换成向量

TaggedDocument是一个说明性类，用于表示Doc2Vec可以作为文本示例的对象。您不需要使用它 - 您只需要提供具有words属性的对象，该属性是字符串标记的列表，以及tags属性，它是标记列表与文档相关联。 （也就是说，您可以将文字示例提供为“TaggedDocument形状”或“鸭子类型”的对象。）

TaggedLineDocument是一个实用程序类，用于获取每行包含一个文档的文件，其token-wordss已经以空格分隔，并将其转换为TaggedDocument s的可迭代集合，其中每个doc将其唯一标记作为整数行号。因此，它是将文本流式传输到Doc2Vec的最小示例，对于单个doc-per-line文本文件的常见情况作为输入，并且不需要自定义的每文档标签/ ID。


https://github.com/HelWireless/doc2vec_milvus_tutorial/blob/master/5_30_doc2vec_milvus/data_processed.ipynb



句子转换为向量


作者：知乎用户
链接：https://www.zhihu.com/question/48013814/answer/344364355
来源：知乎
著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。

可以用gensim包的doc2vec来将一个句子转化为句向量，具体使用代码如下：
```python
from gensim.models import Doc2Vec

# doc2vec parameters
vector_size = 300  # 300维
window_size = 15
min_count = 1
sampling_threshold = 1e-5
negative_size = 5
train_epoch = 100
dm = 0  # 0 = dbow; 1 = dmpv
worker_count = 8  # number of parallel processes

# input corpus
train_corpus = "../train_data/train_docs.txt"

def train(run_dir):
    # 训练Doc2Vec，并保存模型
    docs = gensim.models.doc2vec.TaggedLineDocument(train_corpus)
    '''
    dm: 训练算法：默认为1，指DM；dm=0，则使用DBOW
    dm_mean：当使用DM训练算法时，对上下文向量相加（默认为0）；若设为1，则求均值
    dm_concat：默认为0，当设为1时，在使用DM训练算法时，直接将上下文向量和Doc向量拼接
    dbow_words：当设为1时，则在训练doc_vector（DBOW) 的同时训练word_vector; 默认为0，只训练doc_vector，速度更快
    '''
    model = Doc2Vec(docs, size=vector_size, window=window_size, min_count=min_count, sample=sampling_threshold,
                    workers=worker_count, hs=0, dm=dm, negative=negative_size, dbow_words=1, dm_concat=1,
                    iter=train_epoch)
    model.save(os.path.join(run_dir, saved_model))  # save dov2vec
    model.wv.save_word2vec_format(os.path.join(run_dir, word_vector_path), binary=False)  # save word2vec


# 利用gensim 直接生成文档向量

def gen_d2v_corpus(self, lines):

    with open("./data/ques2_result.txt", "wb") as fw:
        for line in lines:
            fw.write(" ".join(jieba.lcut(line)) + "\n")

    sents = doc2vec.TaggedLineDocument("./data/ques2_result.txt")
    model = doc2vec.Doc2Vec(sents, size = 50, window = 5, alpha = 0.015)
    model.train(sents)

    corpus = model.docvecs
    np.save("./output/d2v.corpus.npy", corpus)

    return np.asarray(corpus)
```

https://www.zhihu.com/question/48013814


https://www.yuanmas.com/info/8VaPm97yrq.html

https://gist.github.com/Tedko/1bdb1dc7f8654c98453c42692862d200

# 搜狐新闻文本分类：机器学习大乱斗

https://www.it610.com/article/1297554426529128448.htm


# 推荐系统Rerank

推荐系统重新排序

https://www.jiqizhixin.com/articles/2019-11-12-16


# A/B测试

https://blog.back4app.com/zh/firebase%E6%9C%80%E4%BD%B3%E6%9B%BF%E4%BB%A3%E5%93%81/

推荐召回测试

https://tech.meituan.com/2017/03/24/travel-recsys.html

# 全民K歌召回路径

http://www.360doc.com/content/21/0308/21/7673502_965858335.shtml

# 重排排序逻辑

正排-痛点

各模块各自维护相应的离线和在线,稳定性和时效性无法保证;
格式多样: json、msgpack. protobuf等 ;
字段重复、含义不一致,存储不统一: mc、redis等 ;
Trouble- shooting成本比较高;
正排-统一方案

200+字段,由protobuf IDL描述，按"簇"存储;
统一离线刷新框架,保证高时效性和稳定性;
统存储和对外接口 ;
提供完善的debug工具:查询正排内容、更新时间等;


https://studygolang.com/articles/22184

http://www.woshipm.com/pd/4099114.html

# 瑞金主流的模型算法

以上模型和工作，如果让我只挑选最经典的话，我选择：

GBDT+LR
DSSM
Wide and Deep


https://www.zhihu.com/question/352306163


https://www.zybuluo.com/zakexu/note/1747732

# 汽车之家算法debug平台

https://www.infoq.cn/article/87goliaqwzw4mol0g9ke

https://bailingnan.github.io/post/shen-ru-li-jie-xgboost/

## 多路召回排序与融合策略

https://juejin.cn/post/6854573221707317261

xgboost

https://www.cnblogs.com/XDU-Lakers/p/11914401.html

https://bailingnan.github.io/post/shen-ru-li-jie-xgboost/#%E6%95%B0%E6%8D%AE%E5%88%86%E6%9E%90

## 排序算法

https://zhuanlan.zhihu.com/p/138235048


https://lumingdong.cn/learning-to-rank-in-recommendation-system.html