# 数据增量更新方案

# 数据增量更新方案

1. 基于建触发器方式生成增量数据
使用触发器生成增量数据是普遍采取的一种增量抽取机制。该方式是根据抽取要求，在要被抽取的源表上建立3个触发器插入、修改、删除，每当源表中的数据发生变化，就被相应的触发器将变化的数据写入一个增量日志表，ETL的增量抽取则是从增量日志表中而不是直接在源表中抽取数据，同时增量日志表中抽取过的数据要及时被标记或删除。

2. 基于时间戳方式生成增量数据
时间戳方式是指增量抽取时，抽取进程通过比较系统时间与抽取源表的时间戳字段的值来决定抽取哪些数据。这种方式需要在源表上增加一个时间戳字段，系统中更新修改表数据的时候，同时修改时间戳字段的值。

3. 基于全表比对方式生成增量数据
全表比对即在增量抽取时，ETL进程逐条比较源表和目标表的记录，将新增和修改的记录读取出来。

优化之后的全部比对方式是采用MD5校验码，需要事先为要抽取的表建立一个结构类似的MD5临时表，该临时表记录源表的主键值以及根据源表所有字段的数据计算出来的MD5校验码，每次进行数据抽取时，对源表和MD5临时表进行MD5校验码的比对，如有不同，进行update操作：如目标表没有存在该主键值，表示该记录还没有，则进行insert操作。然后，还需要对在源表中已不存在而目标表仍保留的主键值，执行delete操作。

4. 基于日志表方式生成增量数据
对于建立了业务系统的生产数据库，可以在数据库中创建业务日志表，当特定需要监控的业务数据发生变化时，由相应的业务系统程序模块来更新维护日志表内容。增量抽取时，通过读日志表数据决定加载哪些数据及如何加载。日志表的维护需要由业务系统程序用代码来完成。

https://zhuanlan.zhihu.com/p/362471672

# 实战

## 全量数据同步改为增量同步

https://blog.csdn.net/oTengYue/article/details/53516028

# 网友给出的方案
oyljerry
oyljerry 2018-03-06 09:05
百万的数据量，直接用数据库处理好了。对于判断的数据列，建个索引，插入数据的时候判断一下


wlsspeed
wlsspeed 2018-03-06 12:02
具体的最佳解决方案，取决于你的实际情况。
比如，
1、第二次导入时能够按照第一次的顺序导入，那么直接进行按照顺序比较，对于差异部分进行更新，相同部分不更新即可。
2、数据量总量在1千万以下，用楼上的数据库处理机制直接处理。
3、前面的100万的导入时间比较短，或者成本比较低，都可以接受，就可以全清掉，重新倒，而不需要判断以前有什么数据。
等等等等。


# 产品中数据增量更新的方案

## qlik

https://help.qlik.com/zh-CN/sense/May2021/Subsystems/Hub/Content/Sense_Hub/LoadData/use-QVD-files-incremental-load.htm

## FineBI

https://help.fanruan.com/finebi/doc-view-92.html

# 邮件/日历/SNS等客户端的增量更新方案

在邮件/日历/SNS等客户端里，客户端数据要不断与服务端进行数据同步，在同步过程中，只拉取有修改的数据，称为增量更新，增量更新方案一般有两种，一是对比，二是日志。

## 对比
对比就是客户端请求服务端所有关键数据，跟本地已有的数据进行对比，筛选出增删改的数据进行更新。
邮件协议IMAP，日历协议CalDAV就是用这种方式做增量更新，IMAP并没有做上述的优化，在判断邮件有没有更新时只能乖乖把所有数据请求回来对比，数据是XML，算是相当低效的协议。CalDAV给每个日历事件加了上述的tag，直接对比即可知道是否需要更新。



https://blog.cnbang.net/tech/2258/