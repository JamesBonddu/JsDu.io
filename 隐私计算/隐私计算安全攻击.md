# 隐私计算安全分类

https://iami.xyz/Privacy-Computing-And-Data-Security/

https://www.cnblogs.com/MaplesWCT/p/16211902.html


FL生态系统的脆弱性来源

https://blog.csdn.net/A33280000f/article/details/120136848

https://juejin.cn/post/7045096766483464228

https://www.bilibili.com/video/BV1UW4y1m7Eg/

https://www.cnblogs.com/MaplesWCT/p/16211925.html

https://www.anquanke.com/post/id/255550#h2-2

https://www.bilibili.com/video/BV19Z4y167sq/


## 推理攻击技术

推理攻击旨在通过反转机器学习模型中的信息流，以方便攻击者洞察到那些并未显式共享的模型。从统计学的角度来说，由于私有的机密数据往往与公开发布数据有着潜在的相关性，而机器学习的各种分类器(classifier)具有捕捉到此类统计相关性的能力，因此推理攻击会给个人和系统构成严重的隐私和安全威胁。

通常，此类攻击会包括三种类型：

- 成员推理攻击(Membership Inference Attack，MIA，译者注：通过识别目标模型在行为上的差异，来区分其中的成员和非成员)。
- 属性推理攻击(Property Inference Attack，PIA，译者注：攻击者利用公开可见的属性和结构，推理出隐蔽或不完整的属性数据)。
- 恢复训练数据(Recovery training data，译者注：试图恢复和重建那些在训练过程中使用过的数据)。

https://cn-sec.com/archives/1161303.html

https://blog.csdn.net/qq_44775223/article/details/126980478

https://blog.csdn.net/weixin_42468475/article/details/123028846

https://www.51cto.com/article/699856.html


wget -c https://mirrors.tuna.tsinghua.edu.cn/anaconda/miniconda/Miniconda3-latest-Linux-x86_64.sh
wget -c https://mirrors.bfsu.edu.cn/anaconda/miniconda/Miniconda3-latest-Linux-x86_64.sh

# 攻击方式

## 投毒攻击
主要投毒方式
1. 数据投毒
   数据投毒是指攻击者通过对训练集中的样本进行污染，如添加错误的标签或有偏差的数据，降低数据的质量，从而影响最后训练出来的模型，破坏其可用性或完整性。例如，攻击者使学习模型的参数值接近他所期望的值，同时使模型输出对某些测试样本的错误预测。有研究显示，采用混合辅助注入策略，通过注入少量有毒样本到训练集就获得了90%以上的攻击成功率。也有研究指出，针对支持向量机算法（SVM）产生的优化梯度，预测其目标函数的变化方向，使用梯度上升策略显著提高了SVM分类器的错误率。
2. 模型投毒
    模型投毒不同于数据投毒，攻击者不直接对训练数据进行操作，而是发送错误的参数或是损坏的模型来破坏全局聚合期间的学习过程，比如控制某些参与方传给服务器的更新参数，从而影响整个学习模型参数的变化方向，减慢模型的收敛速度，甚至破坏整体模型的正确性，严重影响模型的性能，有研究者只假设了一个恶意代理（参与方），就实现了对整体模型的隐蔽性的攻击，使得目标模型无法对某类数据正确分类。

https://www.anquanke.com/post/id/205097

分布式投毒攻击
https://www.anquanke.com/post/id/211500

https://blog.csdn.net/w18727139695/article/details/123727138

https://blog.csdn.net/OpenMpc/article/details/126135413

https://blog.csdn.net/qq_36160277/article/details/122335691

https://www.dounaite.com/article/6313eb5af4ab41be48e8645c.html

https://www.cnblogs.com/MaplesWCT/p/16211902.html


## 对抗攻击

对抗攻击是指恶意构造输入样本，导致模型以高置信度输出错误结果。这种通过在原始样本中添加扰动而产生的输入样本称为对抗样本。
1. 黑盒攻击
   攻击者不知道任何模型的信息，只能跟模型互动，给模型提供输入然后观察它的输出，这种情形下的对矿攻击属于黑盒攻击
2. 白盒攻击
    攻击者可以把所需的干扰看作一个优化问题计算出来。这种情况下的对抗攻击属于白盒攻击

## 隐私泄漏

联邦学习方式允许参与方在本地进行数据训练，各参与方之间是独立进行的,可以保证一定的隐私安全，但这种安全并不是绝对安全，仍存在隐私泄露的风险。比如恶意的参与方可以从共享的参数中推理出其他参与方的敏感信息。

1. 模型提取
    通过模型提取攻击，攻击者试图窃取模型的参数和超参数。比如恶意的参与方可以对共享模型进行预测查询，然后提取训练完整的模型。有研究表明，攻击者针对BigML和Amazon机器学习在线服务进行了攻击，提取了一个几乎完全相同的模型。
2. 模型逆向攻击
    模型逆向攻击，攻击者试图从训练完成的模型中获取训练数据集的统计信息，从而获取用户的隐私信息。模型逆向攻击推断出的训练集的信息，既可以是某个成员是否包含在训练集中，也可以是训练集的一些统计特征。

隐私攻击
1. 成员推断攻击
2. 模型倒推攻击
3. 参数提取攻击

OPacus 利用DP-SGD差分隐私随机梯度下降.

## 后门攻击 backdoor

https://arxiv.org/pdf/1807.00459.pdf

https://github.com/FedML-AI/FedML/blob/master/research/Awesome-Federated-Learning.md#trustworthiness-security-privacy-fairness-incentive-mechanism-etc-88

https://blog.csdn.net/weixin_43682519/article/details/109096254

https://github.com/THUYimingLi/backdoor-learning-resources

## 可信执行环境 Trusted Execution Environment TEE
Intel SGX 可信应用程序的执行流程
1. 硬件设备注册
2. 可信应用部署
3. 可信应用程序调用

优势:
1. 计算性能高
2. 可扩展性强
3. 应用程序可验证

安全风险:
1. 可能受到侧信道攻击
   假设攻击者知道运行Enclave平台的硬件配置, 特性和性能, 比如 CPU， TLB，DRAM 等且知道内存布局, 内存布局包括虚拟地址, 物理地址, 以及他们之间的映射关系。
   侧信道攻击主要手段就是通过攻击面获取数据, 推导获得控制流和数据流信息, 最终获得Enclave保护的代码和数据.
2. 用户必须相信硬件厂商或平台, 一定程度上依赖于远程硬件鉴权服务器是否诚实, 是否被破解, 是否有最新的黑名单信息.
3. 漏洞曝光后很难及时修复
4. 密钥管理可用性问题

https://cn-sec.com/archives/1312944.html

# 攻击和防御方法

https://github.com/innovation-cat/Awesome-Federated-Machine-Learning


https://learn.microsoft.com/zh-cn/security/engineering/bug-bar-aiml


https://github.com/404notf0und/AI-for-Security-Learning


# 横向联邦学习的问题

https://juejin.cn/post/7117997783474257927#heading-1


# 其中的协议
https://www.8btc.com/media/6721506
